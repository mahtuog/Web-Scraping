{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urllib.request.urlopen('https://www.appliedaicourse.com/course/applied-ai-course-online/').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(page,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = soup.findAll('li',{'class':'section'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "\u001b[1m Section 0 How to utilise Appliedaicourse  0/0\n",
      "\u001b[0m* How to Learn from  Appliedaicourse \n",
      "**************************************************\n",
      "\u001b[1m Section 1 Python for Data Science Introduction  0/1\n",
      "\u001b[0m* Python, Anaconda and relevant packages installations \n",
      "\u001b[0m* Why learn Python? \n",
      "\u001b[0m* Keywords and identifiers \n",
      "\u001b[0m* comments, indentation and statements \n",
      "\u001b[0m* Variables and data types in Python \n",
      "\u001b[0m* Standard Input and Output \n",
      "\u001b[0m* Operators \n",
      "\u001b[0m* Control flow: if else \n",
      "\u001b[0m* Control flow: while loop \n",
      "\u001b[0m* Control flow: for loop \n",
      "\u001b[0m* Control flow: break and continue \n",
      "**************************************************\n",
      "\u001b[1m Section 2 Python for Data Science: Data Structures  0/6\n",
      "\u001b[0m* Lists \n",
      "\u001b[0m* Tuples part 1 \n",
      "\u001b[0m* Tuples part-2 \n",
      "\u001b[0m* Sets \n",
      "\u001b[0m* Dictionary \n",
      "\u001b[0m* Strings \n",
      "**************************************************\n",
      "\u001b[1m Section 3 Python for Data Science: Functions  0/10\n",
      "\u001b[0m* Introduction \n",
      "\u001b[0m* Types of functions \n",
      "\u001b[0m* Function arguments \n",
      "\u001b[0m* Recursive functions \n",
      "\u001b[0m* Lambda functions \n",
      "\u001b[0m* Modules \n",
      "\u001b[0m* Packages \n",
      "\u001b[0m* File Handling \n",
      "\u001b[0m* Exception Handling \n",
      "\u001b[0m* Debugging Python \n",
      "**************************************************\n",
      "\u001b[1m Section 4 Python for Data Science: Numpy  0/2\n",
      "\u001b[0m* Numpy Introduction \n",
      "\u001b[0m* Numerical operations on Numpy \n",
      "**************************************************\n",
      "\u001b[1m Section 5 Python for Data Science: Matplotlib  0/1\n",
      "\u001b[0m* Getting started with Matplotlib \n",
      "**************************************************\n",
      "\u001b[1m Section 6 Python for Data Science: Pandas  0/3\n",
      "\u001b[0m* Getting started with pandas \n",
      "\u001b[0m* Data Frame Basics \n",
      "\u001b[0m* Key Operations on Data Frames \n",
      "**************************************************\n",
      "\u001b[1m Section 7 Python for Data Science: Computational Complexity  0/4\n",
      "\u001b[0m* Space and Time Complexity: Find largest number in a list \n",
      "\u001b[0m* Binary search \n",
      "\u001b[0m* Find elements common in two lists \n",
      "\u001b[0m* Find elements common in two lists using a Hashtable/Dict \n",
      "**************************************************\n",
      "\u001b[1m Section 8 Plotting for exploratory data analysis (EDA)  0/0\n",
      "\u001b[0m* Introduction to IRIS dataset and 2D scatter plot \n",
      "\u001b[0m* 3D scatter plot \n",
      "\u001b[0m* Pair plots \n",
      "\u001b[0m* Limitations of Pair Plots \n",
      "\u001b[0m* Histogram and Introduction to PDF(Probability Density Function) \n",
      "\u001b[0m* Univariate Analysis using PDF \n",
      "\u001b[0m* CDF(Cumulative Distribution Function) \n",
      "\u001b[0m* Mean, Variance and Standard Deviation \n",
      "\u001b[0m* Median \n",
      "\u001b[0m* Percentiles and Quantiles \n",
      "\u001b[0m* IQR(Inter Quartile Range) and MAD(Median Absolute Deviation) \n",
      "\u001b[0m* Box-plot with Whiskers \n",
      "\u001b[0m* Violin Plots \n",
      "\u001b[0m* Summarizing Plots, Univariate, Bivariate and Multivariate analysis \n",
      "\u001b[0m* Multivariate Probability Density, Contour Plot \n",
      "\u001b[0m* Exercise: Perform EDA on Haberman dataset \n",
      "**************************************************\n",
      "\u001b[1m Section 9 Linear Algebra  0/1\n",
      "\u001b[0m* Why learn it ? \n",
      "\u001b[0m* Introduction to Vectors(2-D, 3-D, n-D) , Row Vector and Column Vector \n",
      "\u001b[0m* Dot Product and Angle between 2 Vectors \n",
      "\u001b[0m* Projection and Unit Vector \n",
      "\u001b[0m* Equation of a line (2-D), Plane(3-D) and Hyperplane (n-D), Plane Passing through origin, Normal to a Plane \n",
      "\u001b[0m* Distance of a point from a Plane/Hyperplane, Half-Spaces \n",
      "\u001b[0m* Equation of a Circle (2-D), Sphere (3-D) and Hypersphere (n-D) \n",
      "\u001b[0m* Equation of an Ellipse (2-D), Ellipsoid (3-D) and Hyperellipsoid (n-D) \n",
      "\u001b[0m* Square ,Rectangle \n",
      "\u001b[0m* Hyper Cube,Hyper Cuboid \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 10 Probability and Statistics  0/26\n",
      "\u001b[0m* Introduction to Probability and Statistics \n",
      "\u001b[0m* Population and Sample \n",
      "\u001b[0m* Gaussian/Normal Distribution and its PDF(Probability Density Function) \n",
      "\u001b[0m* CDF(Cumulative Distribution function) of Gaussian/Normal distribution \n",
      "\u001b[0m* Symmetric distribution, Skewness and Kurtosis \n",
      "\u001b[0m* Standard normal  variate (Z) and standardization \n",
      "\u001b[0m* Kernel density estimation \n",
      "\u001b[0m* Sampling distribution & Central Limit theorem \n",
      "\u001b[0m* Q-Q plot:How to test if a random variable is normally distributed or not? \n",
      "\u001b[0m* Discrete and Continuous Uniform distributions \n",
      "\u001b[0m* How to randomly sample data points (Uniform Distribution) \n",
      "\u001b[0m* Bernoulli and Binomial Distribution \n",
      "\u001b[0m* Log Normal Distribution \n",
      "\u001b[0m* Power law distribution \n",
      "\u001b[0m* Box cox transform \n",
      "\u001b[0m* Co-variance \n",
      "\u001b[0m* Pearson Correlation Coefficient \n",
      "\u001b[0m* Spearman Rank Correlation Coefficient \n",
      "\u001b[0m* Correlation vs Causation \n",
      "\u001b[0m* Confidence interval (C.I) Introduction \n",
      "\u001b[0m* Computing confidence interval given the underlying distribution \n",
      "\u001b[0m* C.I for mean of a normal random variable \n",
      "\u001b[0m* Confidence interval using bootstrapping \n",
      "\u001b[0m* Hypothesis testing methodology, Null-hypothesis, p-value \n",
      "\u001b[0m* Resampling and permutation test \n",
      "\u001b[0m* K-S Test for similarity of  two distributions \n",
      "\u001b[0m* Code Snippet K-S Test \n",
      "\u001b[0m* Hypothesis Testing Intution with coin toss example \n",
      "\u001b[0m* Hypothesis testing Mean differences Example \n",
      "\u001b[0m* Resampling and Permutation test for Mean difference example \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 11 Interview Questions on Probability and statistics  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 12 Dimensionality reduction and Visualization:  0/0\n",
      "\u001b[0m* What is Dimensionality reduction? \n",
      "\u001b[0m* Row Vector and Column Vector \n",
      "\u001b[0m* How to represent a data set? \n",
      "\u001b[0m* How to represent a dataset as a Matrix. \n",
      "\u001b[0m* Data Preprocessing: Feature Normalisation \n",
      "\u001b[0m* Mean of a data matrix \n",
      "\u001b[0m* Data Preprocessing: Column Standardization \n",
      "\u001b[0m* Co-variance of a Data Matrix \n",
      "\u001b[0m* MNIST dataset (784 dimensional) \n",
      "\u001b[0m* Code to Load MNIST Data Set \n",
      "**************************************************\n",
      "\u001b[1m Section 13 PCA(principal component analysis)  0/0\n",
      "\u001b[0m* Why learn PCA? \n",
      "\u001b[0m* Geometric intuition of PCA \n",
      "\u001b[0m* Mathematical objective function of PCA \n",
      "\u001b[0m* Alternative formulation of PCA: Distance minimization \n",
      "\u001b[0m* Eigen values and Eigen vectors (PCA): Dimensionality reduction \n",
      "\u001b[0m* PCA for Dimensionality Reduction and Visualization \n",
      "\u001b[0m* Visualize MNIST dataset \n",
      "\u001b[0m* Limitations of PCA \n",
      "\u001b[0m* PCA Code example \n",
      "\u001b[0m* PCA for dimensionality reduction (not-visualization) \n",
      "**************************************************\n",
      "\u001b[1m Section 14 (t-SNE)T-distributed Stochastic Neighbourhood Embedding  0/1\n",
      "\u001b[0m* What is t-SNE? \n",
      "\u001b[0m* Neighborhood of a point, Embedding \n",
      "\u001b[0m* Geometric intuition of t-SNE \n",
      "\u001b[0m* Crowding Problem \n",
      "\u001b[0m* How to apply t-SNE and interpret its output \n",
      "\u001b[0m* t-SNE on MNIST \n",
      "\u001b[0m* Code example of t-SNE \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 15 Interview Questions on Dimensionality Reduction  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 16 Real world problem: Predict rating given product reviews on Amazon  0/18\n",
      "\u001b[0m* Dataset overview: Amazon Fine Food reviews(EDA) \n",
      "\u001b[0m* Data Cleaning: Deduplication \n",
      "\u001b[0m* Why convert text to a vector? \n",
      "\u001b[0m* Bag of Words (BoW) \n",
      "\u001b[0m* Text Preprocessing: Stemming, Stop-word removal, Tokenization, Lemmatization. \n",
      "\u001b[0m* uni-gram, bi-gram, n-grams. \n",
      "\u001b[0m* tf-idf (term frequency- inverse document frequency) \n",
      "\u001b[0m* Why use log in IDF? \n",
      "\u001b[0m* Word2Vec. \n",
      "\u001b[0m* Avg-Word2Vec, tf-idf weighted Word2Vec \n",
      "\u001b[0m* Bag of Words( Code Sample) \n",
      "\u001b[0m* Text Preprocessing( Code Sample) \n",
      "\u001b[0m* Bi-Grams and n-grams (Code Sample) \n",
      "\u001b[0m* TF-IDF (Code Sample) \n",
      "\u001b[0m* Word2Vec (Code Sample) \n",
      "\u001b[0m* Avg-Word2Vec and TFIDF-Word2Vec (Code Sample) \n",
      "\u001b[0m* Exercise: t-SNE visualization of Amazon reviews with polarity based color-coding \n",
      "\u001b[0m* Assignment \n",
      "**************************************************\n",
      "\u001b[1m Section 17 Classification And Regression Models: K-Nearest Neighbors  0/33\n",
      "\u001b[0m* How “Classification” works? \n",
      "\u001b[0m* Data matrix notation \n",
      "\u001b[0m* Classification vs Regression (examples) \n",
      "\u001b[0m* K-Nearest Neighbours Geometric intuition with a toy example \n",
      "\u001b[0m* Failure cases of KNN \n",
      "\u001b[0m* Distance measures: Euclidean(L2) , Manhattan(L1), Minkowski,  Hamming \n",
      "\u001b[0m* Cosine Distance & Cosine Similarity \n",
      "\u001b[0m* How to measure the effectiveness of k-NN? \n",
      "\u001b[0m* Test/Evaluation time and space complexity \n",
      "\u001b[0m* KNN Limitations \n",
      "\u001b[0m* Decision surface for K-NN as K changes \n",
      "\u001b[0m* Overfitting and Underfitting \n",
      "\u001b[0m* Need for Cross validation \n",
      "\u001b[0m* K-fold cross validation \n",
      "\u001b[0m* Visualizing train, validation and test datasets \n",
      "\u001b[0m* How to determine overfitting and underfitting? \n",
      "\u001b[0m* Time based splitting \n",
      "\u001b[0m* k-NN for regression \n",
      "\u001b[0m* Weighted k-NN \n",
      "\u001b[0m* Voronoi diagram \n",
      "\u001b[0m* Binary search tree \n",
      "\u001b[0m* How to build a kd-tree \n",
      "\u001b[0m* Find nearest neighbours using kd-tree \n",
      "\u001b[0m* Limitations of Kd tree \n",
      "\u001b[0m* Extensions \n",
      "\u001b[0m* Hashing vs LSH \n",
      "\u001b[0m* LSH for cosine similarity \n",
      "\u001b[0m* LSH for euclidean distance \n",
      "\u001b[0m* Probabilistic class label \n",
      "\u001b[0m* Code Sample:Decision boundary . \n",
      "\u001b[0m* Code Sample:Cross Validation \n",
      "\u001b[0m* Exercise: Apply k-NN on Amazon reviews dataset \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 18 Interview Questions on K-NN(K Nearest Neighbour)  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 19 Classification algorithms in various situations  0/21\n",
      "\u001b[0m* Introduction \n",
      "\u001b[0m* Imbalanced vs balanced dataset \n",
      "\u001b[0m* Multi-class classification \n",
      "\u001b[0m* k-NN, given a distance or similarity matrix \n",
      "\u001b[0m* Train and test set differences \n",
      "\u001b[0m* Impact of outliers \n",
      "\u001b[0m* Local outlier Factor (Simple solution :Mean distance to Knn) \n",
      "\u001b[0m* k distance \n",
      "\u001b[0m* Reachability-Distance(A,B) \n",
      "\u001b[0m* Local reachability-density(A) \n",
      "\u001b[0m* Local outlier Factor(A) \n",
      "\u001b[0m* Impact of Scale & Column standardization \n",
      "\u001b[0m* Interpretability \n",
      "\u001b[0m* Feature Importance and Forward Feature selection \n",
      "\u001b[0m* Handling categorical and numerical features \n",
      "\u001b[0m* Handling missing values by imputation \n",
      "\u001b[0m* curse of dimensionality \n",
      "\u001b[0m* Bias-Variance tradeoff \n",
      "\u001b[0m* Intuitive understanding of bias-variance. \n",
      "\u001b[0m* Revision Questions \n",
      "\u001b[0m* best and wrost case of algorithm \n",
      "**************************************************\n",
      "\u001b[1m Section 20 Performance measurement of models  0/9\n",
      "\u001b[0m* Accuracy \n",
      "\u001b[0m* Confusion matrix, TPR, FPR, FNR, TNR \n",
      "\u001b[0m* Precision and recall, F1-score \n",
      "\u001b[0m* Receiver Operating Characteristic Curve (ROC) curve and AUC \n",
      "\u001b[0m* Log-loss \n",
      "\u001b[0m* R-Squared/Coefficient of determination \n",
      "\u001b[0m* Median absolute deviation (MAD) \n",
      "\u001b[0m* Revision Questions \n",
      "\u001b[0m* Distribution of errors \n",
      "**************************************************\n",
      "\u001b[1m Section 21 Interview Questions on Performance Measurement Models  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 22 Naive Bayes  0/22\n",
      "\u001b[0m* Conditional probability \n",
      "\u001b[0m* Independent vs Mutually exclusive events \n",
      "\u001b[0m* Bayes Theorem with examples \n",
      "\u001b[0m* Exercise problems on Bayes Theorem \n",
      "\u001b[0m* Naive Bayes algorithm \n",
      "\u001b[0m* Toy example: Train and test stages \n",
      "\u001b[0m* Naive Bayes on Text data \n",
      "\u001b[0m* Laplace/Additive Smoothing \n",
      "\u001b[0m* Log-probabilities for numerical stability \n",
      "\u001b[0m* Bias and Variance tradeoff \n",
      "\u001b[0m* Feature importance and interpretability \n",
      "\u001b[0m* Imbalanced data \n",
      "\u001b[0m* Outliers \n",
      "\u001b[0m* Missing values \n",
      "\u001b[0m* Handling Numerical features (Gaussian NB) \n",
      "\u001b[0m* Multiclass classification \n",
      "\u001b[0m* Similarity or Distance matrix \n",
      "\u001b[0m* Large dimensionality \n",
      "\u001b[0m* Best and worst cases \n",
      "\u001b[0m* Code example \n",
      "\u001b[0m* Exercise: Apply Naive Bayes to Amazon reviews \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 23 Logistic Regression  0/18\n",
      "\u001b[0m* Geometric intuition of Logistic Regression \n",
      "\u001b[0m* Sigmoid function: Squashing \n",
      "\u001b[0m* Mathematical formulation of Objective function \n",
      "\u001b[0m* Weight vector \n",
      "\u001b[0m* L2 Regularization: Overfitting and Underfitting \n",
      "\u001b[0m* L1 regularization and sparsity \n",
      "\u001b[0m* Probabilistic Interpretation: Gaussian Naive Bayes \n",
      "\u001b[0m* Loss minimization interpretation \n",
      "\u001b[0m* hyperparameters and random search \n",
      "\u001b[0m* Column Standardization \n",
      "\u001b[0m* Feature importance and Model interpretability \n",
      "\u001b[0m* Collinearity of features \n",
      "\u001b[0m* Test/Run time space and time complexity \n",
      "\u001b[0m* Real world cases \n",
      "\u001b[0m* Non-linearly separable data & feature engineering \n",
      "\u001b[0m* Code sample: Logistic regression, GridSearchCV, RandomSearchCV \n",
      "\u001b[0m* Exercise: Apply Logistic regression to Amazon reviews dataset. \n",
      "\u001b[0m* Extensions to Generalized linear models \n",
      "**************************************************\n",
      "\u001b[1m Section 24 Linear Regression  0/4\n",
      "\u001b[0m* Geometric intuition of Linear Regression \n",
      "\u001b[0m* Mathematical formulation \n",
      "\u001b[0m* Real world Cases \n",
      "\u001b[0m* Code sample for Linear Regression \n",
      "**************************************************\n",
      "\u001b[1m Section 25 Solving Optimization Problems  0/13\n",
      "\u001b[0m* Differentiation \n",
      "\u001b[0m* Revision Questions \n",
      "\u001b[0m* Online differentiation tools \n",
      "\u001b[0m* Maxima and Minima \n",
      "\u001b[0m* Vector calculus: Grad \n",
      "\u001b[0m* Gradient descent: geometric intuition \n",
      "\u001b[0m* Learning rate \n",
      "\u001b[0m* Gradient descent for linear regression \n",
      "\u001b[0m* SGD algorithm \n",
      "\u001b[0m* Constrained Optimization & PCA \n",
      "\u001b[0m* Logistic regression formulation revisited \n",
      "\u001b[0m* Why L1 regularization creates sparsity? \n",
      "\u001b[0m* Exercise: Implement SGD for linear regression \n",
      "\u001b[0m* Revision questions \n",
      "**************************************************\n",
      "\u001b[1m Section 26 Interview Questions on Logistic Regression and Linear Regression  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 27 Support Vector Machines (SVM)  0/16\n",
      "\u001b[0m* Geometric Intution \n",
      "\u001b[0m* Mathematical derivation \n",
      "\u001b[0m* Why we take values +1 and and -1 for Support vector planes \n",
      "\u001b[0m* Loss function (Hinge Loss) based interpretation \n",
      "\u001b[0m* Dual form of SVM formulation \n",
      "\u001b[0m* kernel trick \n",
      "\u001b[0m* kernel Polynomial \n",
      "\u001b[0m* RBF-Kernel \n",
      "\u001b[0m* Domain specific Kernels \n",
      "\u001b[0m* Train and run time complexities \n",
      "\u001b[0m* nu-SVM: control errors and support vectors \n",
      "\u001b[0m* SVM Regression \n",
      "\u001b[0m* Cases \n",
      "\u001b[0m* Code Sample \n",
      "\u001b[0m* Exercise: Apply SVM to Amazon reviews dataset \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 28 Interview Questions on Support Vector Machine  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 29 Decision Trees  0/16\n",
      "\u001b[0m* Geometric Intuition of decision tree: Axis parallel hyperplanes \n",
      "\u001b[0m* Sample Decision tree \n",
      "\u001b[0m* Building a decision Tree:Entropy \n",
      "\u001b[0m* Building a decision Tree:Information Gain \n",
      "\u001b[0m* Building a decision Tree: Gini Impurity \n",
      "\u001b[0m* Building a decision Tree: Constructing a DT \n",
      "\u001b[0m* Building a decision Tree: Splitting numerical features \n",
      "\u001b[0m* Feature standardization \n",
      "\u001b[0m* Building a decision Tree:Categorical features with many possible values \n",
      "\u001b[0m* Overfitting and Underfitting \n",
      "\u001b[0m* Train and Run time complexity \n",
      "\u001b[0m* Regression using Decision Trees \n",
      "\u001b[0m* Cases \n",
      "\u001b[0m* Code Samples \n",
      "\u001b[0m* Exercise: Decision Trees on Amazon reviews dataset \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 30 Interview Questions on decision Trees  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 31 Ensemble Models  0/20\n",
      "\u001b[0m* What are ensembles? \n",
      "\u001b[0m* Bootstrapped Aggregation (Bagging) Intuition \n",
      "\u001b[0m* Random Forest and their construction \n",
      "\u001b[0m* Bias-Variance tradeoff \n",
      "\u001b[0m* Train and run time complexity \n",
      "\u001b[0m* Bagging:Code Sample \n",
      "\u001b[0m* Extremely randomized trees \n",
      "\u001b[0m* Random Tree :Cases \n",
      "\u001b[0m* Boosting Intuition \n",
      "\u001b[0m* Residuals, Loss functions and gradients \n",
      "\u001b[0m* Gradient Boosting \n",
      "\u001b[0m* Regularization by Shrinkage \n",
      "\u001b[0m* Train and Run time complexity \n",
      "\u001b[0m* XGBoost: Boosting + Randomization \n",
      "\u001b[0m* AdaBoost: geometric intuition \n",
      "\u001b[0m* Stacking models \n",
      "\u001b[0m* Cascading classifiers \n",
      "\u001b[0m* Kaggle competitions vs Real world \n",
      "\u001b[0m* Exercise: Apply GBDT and RF to Amazon reviews dataset. \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 32 Featurization and Feature engineering.  0/18\n",
      "\u001b[0m* Introduction \n",
      "\u001b[0m* Moving window for Time Series Data \n",
      "\u001b[0m* Fourier decomposition \n",
      "\u001b[0m* Deep learning features: LSTM \n",
      "\u001b[0m* Image histogram \n",
      "\u001b[0m* Keypoints: SIFT. \n",
      "\u001b[0m* Deep learning features: CNN \n",
      "\u001b[0m* Relational data \n",
      "\u001b[0m* Graph data \n",
      "\u001b[0m* Indicator variables \n",
      "\u001b[0m* Feature binning \n",
      "\u001b[0m* Interaction variables \n",
      "\u001b[0m* Mathematical transforms \n",
      "\u001b[0m* Model specific featurizations \n",
      "\u001b[0m* Feature orthogonality \n",
      "\u001b[0m* Domain specific featurizations \n",
      "\u001b[0m* Feature slicing \n",
      "\u001b[0m* Kaggle Winners solutions \n",
      "**************************************************\n",
      "\u001b[1m Section 33 Miscellaneous Topics  0/8\n",
      "\u001b[0m* Calibration of Models:Need for calibration \n",
      "\u001b[0m* Calibration Plots. \n",
      "\u001b[0m* Platt’s Calibration/Scaling. \n",
      "\u001b[0m* Isotonic Regression \n",
      "\u001b[0m* Code Samples \n",
      "\u001b[0m* Modeling in the presence of outliers: RANSAC \n",
      "\u001b[0m* Productionizing models \n",
      "\u001b[0m* Retraining models periodically. \n",
      "\u001b[0m* A/B testing. \n",
      "\u001b[0m* Data Science Life cycle \n",
      "**************************************************\n",
      "\u001b[1m Section 34 Unsupervised learning/Clustering  0/14\n",
      "\u001b[0m* What is Clustering? \n",
      "\u001b[0m* Unsupervised learning \n",
      "\u001b[0m* Applications \n",
      "\u001b[0m* Metrics for Clustering \n",
      "\u001b[0m* K-Means: Geometric intuition, Centroids \n",
      "\u001b[0m* K-Means: Mathematical formulation: Objective function \n",
      "\u001b[0m* K-Means Algorithm. \n",
      "\u001b[0m* How to initialize: K-Means++ \n",
      "\u001b[0m* Failure cases/Limitations \n",
      "\u001b[0m* K-Medoids \n",
      "\u001b[0m* Determining the right K \n",
      "\u001b[0m* Code Samples \n",
      "\u001b[0m* Time and space complexity \n",
      "\u001b[0m* Exercise: Clustering the Amazon reviews \n",
      "**************************************************\n",
      "\u001b[1m Section 35 Hierarchical clustering Technique  0/7\n",
      "\u001b[0m* Agglomerative & Divisive, Dendrograms \n",
      "\u001b[0m* Agglomerative Clustering \n",
      "\u001b[0m* Proximity methods: Advantages and Limitations. \n",
      "\u001b[0m* Time and Space Complexity \n",
      "\u001b[0m* Limitations of Hierarchical Clustering \n",
      "\u001b[0m* Code sample \n",
      "\u001b[0m* Exercise: Amazon food reviews \n",
      "**************************************************\n",
      "\u001b[1m Section 36 DBSCAN (Density based clustering) Technique  0/11\n",
      "\u001b[0m* Density based clustering \n",
      "\u001b[0m* MinPts and Eps: Density \n",
      "\u001b[0m* Core, Border and Noise points \n",
      "\u001b[0m* Density edge and Density connected points. \n",
      "\u001b[0m* DBSCAN Algorithm \n",
      "\u001b[0m* Hyper Parameters: MinPts and Eps \n",
      "\u001b[0m* Advantages and Limitations of DBSCAN \n",
      "\u001b[0m* Time and Space Complexity \n",
      "\u001b[0m* Code samples. \n",
      "\u001b[0m* Exercise: Amazon Food reviews \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 37 Recommender Systems and Matrix Factorization  0/16\n",
      "\u001b[0m* Problem formulation: IMDB Movie reviews \n",
      "\u001b[0m* Content based vs Collaborative Filtering \n",
      "\u001b[0m* Similarity based Algorithms \n",
      "\u001b[0m* Matrix Factorization: PCA, SVD \n",
      "\u001b[0m* Matrix Factorization: NMF \n",
      "\u001b[0m* Matrix Factorization for Collaborative filtering \n",
      "\u001b[0m* Matrix Factorization for feature engineering \n",
      "\u001b[0m* Clustering as MF \n",
      "\u001b[0m* Hyperparameter tuning \n",
      "\u001b[0m* Matrix Factorization for recommender systems: Netflix Prize Solution \n",
      "\u001b[0m* Cold Start problem \n",
      "\u001b[0m* Word vectors as MF \n",
      "\u001b[0m* Eigen-Faces \n",
      "\u001b[0m* Code example. \n",
      "\u001b[0m* Exercise: Word Vectors using Truncated SVD. \n",
      "\u001b[0m* Revision Questions \n",
      "**************************************************\n",
      "\u001b[1m Section 38 Interview Questions on Recommender Systems and Matrix Factorization.  0/0\n",
      "\u001b[0m* Questions & Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 39 Case Study 2: Personalized Cancer Diagnosis  0/21\n",
      "\u001b[0m* Business/Real world problem : Overview \n",
      "\u001b[0m* Business objectives and constraints. \n",
      "\u001b[0m* ML problem formulation :Data \n",
      "\u001b[0m* ML problem formulation: Mapping real world to ML problem. \n",
      "\u001b[0m* ML problem formulation :Train, CV and Test data construction \n",
      "\u001b[0m* Exploratory Data Analysis:Reading data & preprocessing \n",
      "\u001b[0m* Exploratory Data Analysis:Distribution of Class-labels \n",
      "\u001b[0m* Exploratory Data Analysis: “Random” Model \n",
      "\u001b[0m* Univariate Analysis:Gene feature \n",
      "\u001b[0m* Univariate Analysis:Variation Feature \n",
      "\u001b[0m* Univariate Analysis:Text feature \n",
      "\u001b[0m* Machine Learning Models:Data preparation \n",
      "\u001b[0m* Baseline Model: Naive Bayes \n",
      "\u001b[0m* K-Nearest Neighbors Classification \n",
      "\u001b[0m* Logistic Regression with class balancing \n",
      "\u001b[0m* Logistic Regression without class balancing \n",
      "\u001b[0m* Linear-SVM. \n",
      "\u001b[0m* Random-Forest with one-hot encoded features \n",
      "\u001b[0m* Random-Forest with response-coded features \n",
      "\u001b[0m* Stacking Classifier \n",
      "\u001b[0m* Majority Voting classifier \n",
      "\u001b[0m* Assignments. \n",
      "**************************************************\n",
      "\u001b[1m Section 40 Case study 3:Taxi demand prediction in New York City  0/28\n",
      "\u001b[0m* Business/Real world problem Overview \n",
      "\u001b[0m* Objectives and Constraints \n",
      "\u001b[0m* Mapping to ML problem :Data \n",
      "\u001b[0m* Mapping to ML problem :dask dataframes \n",
      "\u001b[0m* Mapping to ML problem :Fields/Features. \n",
      "\u001b[0m* Mapping to ML problem :Time series forecasting/Regression \n",
      "\u001b[0m* Mapping to ML problem :Performance metrics \n",
      "\u001b[0m* Data Cleaning :Latitude and Longitude data \n",
      "\u001b[0m* Data Cleaning :Trip Duration. \n",
      "\u001b[0m* Data Cleaning :Speed. \n",
      "\u001b[0m* Data Cleaning :Distance. \n",
      "\u001b[0m* Data Cleaning :Fare \n",
      "\u001b[0m* Data Cleaning :Remove all outliers/erroneous points \n",
      "\u001b[0m* Data Preparation:Clustering/Segmentation \n",
      "\u001b[0m* Data Preparation:Time binning \n",
      "\u001b[0m* Data Preparation:Smoothing time-series data. \n",
      "\u001b[0m* Data Preparation:Smoothing time-series data cont.. \n",
      "\u001b[0m* Data Preparation: Time series and Fourier transforms. \n",
      "\u001b[0m* Ratios and previous-time-bin values \n",
      "\u001b[0m* Simple moving average \n",
      "\u001b[0m* Weighted Moving average. \n",
      "\u001b[0m* Exponential weighted moving average \n",
      "\u001b[0m* Results. \n",
      "\u001b[0m* Regression models :Train-Test split & Features \n",
      "\u001b[0m* Linear regression. \n",
      "\u001b[0m* Random Forest regression \n",
      "\u001b[0m* Xgboost Regression \n",
      "\u001b[0m* Model comparison \n",
      "\u001b[0m* Assignment. \n",
      "**************************************************\n",
      "\u001b[1m Section 41 Case Study 4: Microsoft Malware Detection  0/20\n",
      "\u001b[0m* Business/real world problem :Problem definition \n",
      "\u001b[0m* Business/real world problem :Objectives and constraints \n",
      "\u001b[0m* Machine Learning problem mapping :Data overview. \n",
      "\u001b[0m* Machine Learning problem mapping :ML problem \n",
      "\u001b[0m* Machine Learning problem mapping :Train and test splitting \n",
      "\u001b[0m* Exploratory Data Analysis :Class distribution. \n",
      "\u001b[0m* Exploratory Data Analysis :Feature extraction from byte files \n",
      "\u001b[0m* Exploratory Data Analysis :Multivariate analysis of features from byte files \n",
      "\u001b[0m* Exploratory Data Analysis :Train-Test class distribution \n",
      "\u001b[0m* ML models – using byte files only :Random Model \n",
      "\u001b[0m* k-NN \n",
      "\u001b[0m* Logistic regression \n",
      "\u001b[0m* Random Forest and Xgboost \n",
      "\u001b[0m* ASM Files :Feature extraction & Multi-threading. \n",
      "\u001b[0m* File-size feature \n",
      "\u001b[0m* Univariate analysis \n",
      "\u001b[0m* t-SNE analysis. \n",
      "\u001b[0m* ML models on ASM file features \n",
      "\u001b[0m* Models on all features :t-SNE \n",
      "\u001b[0m* Models on all features :RandomForest and Xgboost \n",
      "\u001b[0m* Assignments. \n",
      "**************************************************\n",
      "\u001b[1m Section 42 Case study 5:Netflix Movie Recommendation System  0/27\n",
      "\u001b[0m* Business/Real world problem:Problem definition \n",
      "\u001b[0m* Objectives and constraints \n",
      "\u001b[0m* Mapping to an ML problem:Data overview. \n",
      "\u001b[0m* Mapping to an ML problem:ML problem formulation \n",
      "\u001b[0m* Exploratory Data Analysis:Data preprocessing \n",
      "\u001b[0m* Exploratory Data Analysis:Temporal Train-Test split. \n",
      "\u001b[0m* Exploratory Data Analysis:Preliminary data analysis. \n",
      "\u001b[0m* Exploratory Data Analysis:Sparse matrix representation \n",
      "\u001b[0m* Exploratory Data Analysis:Average ratings for various slices \n",
      "\u001b[0m* Exploratory Data Analysis:Cold start problem \n",
      "\u001b[0m* Computing Similarity matrices:User-User similarity matrix \n",
      "\u001b[0m* Computing Similarity matrices:Movie-Movie similarity \n",
      "\u001b[0m* Computing Similarity matrices:Does movie-movie similarity work? \n",
      "\u001b[0m* ML Models:Surprise library \n",
      "\u001b[0m* Overview of the modelling strategy. \n",
      "\u001b[0m* Data Sampling. \n",
      "\u001b[0m* Google drive with intermediate files \n",
      "\u001b[0m* Featurizations for regression. \n",
      "\u001b[0m* Data transformation for Surprise. \n",
      "\u001b[0m* Xgboost with 13 features \n",
      "\u001b[0m* Surprise Baseline model. \n",
      "\u001b[0m* Xgboost + 13 features +Surprise baseline model \n",
      "\u001b[0m* Surprise KNN predictors \n",
      "\u001b[0m* Matrix Factorization models using Surprise \n",
      "\u001b[0m* SVD ++ with implicit feedback \n",
      "\u001b[0m* Final models with all features and predictors. \n",
      "\u001b[0m* Comparison between various models. \n",
      "\u001b[0m* Comparison between various models. \n",
      "**************************************************\n",
      "\u001b[1m Section 43 Case study 6: Stackoverflow tag predictor  0/17\n",
      "\u001b[0m* Business/Real world problem \n",
      "\u001b[0m* Business objectives and constraints \n",
      "\u001b[0m* Mapping to an ML problem: Data overview \n",
      "\u001b[0m* Mapping to an ML problem:ML problem formulation. \n",
      "\u001b[0m* Mapping to an ML problem:Performance metrics. \n",
      "\u001b[0m* Hamming loss \n",
      "\u001b[0m* EDA:Data Loading \n",
      "\u001b[0m* EDA:Analysis of tags \n",
      "\u001b[0m* EDA:Data Preprocessing \n",
      "\u001b[0m* Data Modeling : Multi label Classification \n",
      "\u001b[0m* Data preparation. \n",
      "\u001b[0m* Train-Test Split \n",
      "\u001b[0m* Featurization \n",
      "\u001b[0m* Logistic regression: One VS Rest \n",
      "\u001b[0m* Sampling data and tags+Weighted models. \n",
      "\u001b[0m* Logistic regression revisited \n",
      "\u001b[0m* Why not use advanced techniques \n",
      "\u001b[0m* Assignments. \n",
      "**************************************************\n",
      "\u001b[1m Section 44 Case Study 7: Quora question Pair Similarity Problem  0/16\n",
      "\u001b[0m* Business/Real world problem : Problem definition \n",
      "\u001b[0m* Business objectives and constraints. \n",
      "\u001b[0m* Mapping to an ML problem : Data overview \n",
      "\u001b[0m* Mapping to an ML problem : ML problem and performance metric. \n",
      "\u001b[0m* Mapping to an ML problem : Train-test split \n",
      "\u001b[0m* EDA: Basic Statistics. \n",
      "\u001b[0m* EDA: Basic Feature Extraction \n",
      "\u001b[0m* EDA: Text Preprocessing \n",
      "\u001b[0m* EDA: Advanced Feature Extraction \n",
      "\u001b[0m* EDA: Feature analysis. \n",
      "\u001b[0m* EDA: Data Visualization: T-SNE. \n",
      "\u001b[0m* EDA: TF-IDF weighted Word2Vec featurization. \n",
      "\u001b[0m* ML Models :Loading Data \n",
      "\u001b[0m* ML Models: Random Model \n",
      "\u001b[0m* ML Models : Logistic Regression and Linear SVM \n",
      "\u001b[0m* ML Models : XGBoost \n",
      "\u001b[0m* Assignments \n",
      "**************************************************\n",
      "\u001b[1m Section 45 Case Study 8: Amazon fashion discovery engine  0/27\n",
      "\u001b[0m* Problem Statement: Recommend similar apparel products in e-commerce using product descriptions and Images \n",
      "\u001b[0m* Plan of action \n",
      "\u001b[0m* Amazon product advertising API \n",
      "\u001b[0m* Data folders and paths \n",
      "\u001b[0m* Overview of the data and Terminology \n",
      "\u001b[0m* Data cleaning and understanding:Missing data in various features \n",
      "\u001b[0m* Understand duplicate rows \n",
      "\u001b[0m* Remove duplicates : Part 1 \n",
      "\u001b[0m* Remove duplicates: Part 2 \n",
      "\u001b[0m* Text Pre-Processing: Tokenization and Stop-word removal \n",
      "\u001b[0m* Stemming \n",
      "\u001b[0m* Text based product similarity :Converting text to an n-D vector: bag of words \n",
      "\u001b[0m* Code for bag of words based product similarity \n",
      "\u001b[0m* TF-IDF: featurizing text based on word-importance \n",
      "\u001b[0m* Code for TF-IDF based product similarity \n",
      "\u001b[0m* Code for IDF based product similarity \n",
      "\u001b[0m* Text Semantics based product similarity: Word2Vec(featurizing text based on semantic similarity) \n",
      "\u001b[0m* Code for Average Word2Vec product similarity \n",
      "\u001b[0m* TF-IDF weighted Word2Vec \n",
      "\u001b[0m* Code for IDF weighted Word2Vec product similarity \n",
      "\u001b[0m* Weighted similarity using brand and color \n",
      "\u001b[0m* Code for weighted similarity \n",
      "\u001b[0m* Building a real world solution \n",
      "\u001b[0m* Deep learning based visual product similarity:ConvNets: How to featurize an image: edges, shapes, parts \n",
      "\u001b[0m* Using Keras + Tensorflow to extract features \n",
      "\u001b[0m* Visual similarity based product similarity \n",
      "\u001b[0m* Measuring goodness of our solution :A/B testing \n",
      "\u001b[0m* Exercise :Build a weighted Nearest neighbor model using Visual, Text, Brand and Color \n",
      "**************************************************\n",
      "\u001b[1m Section 46 Deep Learning:Neural Networks.  0/14\n",
      "\u001b[0m* History of Neural networks and Deep Learning. \n",
      "\u001b[0m* How Biological Neurons work? \n",
      "\u001b[0m* Growth of biological neural networks \n",
      "\u001b[0m* Diagrammatic representation: Logistic Regression and Perceptron \n",
      "\u001b[0m* Multi-Layered Perceptron (MLP). \n",
      "\u001b[0m* Notation \n",
      "\u001b[0m* Training a single-neuron model. \n",
      "\u001b[0m* Training an MLP: Chain Rule \n",
      "\u001b[0m* Training an MLP:Memoization \n",
      "\u001b[0m* Backpropagation. \n",
      "\u001b[0m* Activation functions \n",
      "\u001b[0m* Vanishing Gradient problem. \n",
      "\u001b[0m* Bias-Variance tradeoff. \n",
      "\u001b[0m* Decision surfaces: Playground \n",
      "**************************************************\n",
      "\u001b[1m Section 47 Deep Learning: Deep Multi-layer perceptrons  0/21\n",
      "\u001b[0m* Deep Multi-layer perceptrons:1980s to 2010s \n",
      "\u001b[0m* Dropout layers & Regularization. \n",
      "\u001b[0m* Rectified Linear Units (ReLU). \n",
      "\u001b[0m* Weight initialization. \n",
      "\u001b[0m* Batch Normalization. \n",
      "\u001b[0m* Optimizers:Hill-descent analogy in 2D \n",
      "\u001b[0m* Optimizers:Hill descent in 3D and contours. \n",
      "\u001b[0m* SGD Recap \n",
      "\u001b[0m* Batch SGD with momentum. \n",
      "\u001b[0m* Nesterov Accelerated Gradient (NAG) \n",
      "\u001b[0m* Optimizers:AdaGrad \n",
      "\u001b[0m* Optimizers : Adadelta andRMSProp \n",
      "\u001b[0m* Adam \n",
      "\u001b[0m* Which algorithm to choose when? \n",
      "\u001b[0m* Gradient Checking and clipping \n",
      "\u001b[0m* Softmax and Cross-entropy for multi-class classification. \n",
      "\u001b[0m* How to train a Deep MLP? \n",
      "\u001b[0m* Auto Encoders. \n",
      "\u001b[0m* Word2Vec :CBOW \n",
      "\u001b[0m* Word2Vec: Skip-gram \n",
      "\u001b[0m* Word2Vec :Algorithmic Optimizations. \n",
      "**************************************************\n",
      "\u001b[1m Section 48 Deep Learning: Tensorflow and Keras.  0/14\n",
      "\u001b[0m* Tensorflow and Keras overview \n",
      "\u001b[0m* GPU vs CPU for Deep Learning. \n",
      "\u001b[0m* Google Colaboratory. \n",
      "\u001b[0m* Install TensorFlow \n",
      "\u001b[0m* Online documentation and tutorials \n",
      "\u001b[0m* Softmax Classifier on MNIST dataset. \n",
      "\u001b[0m* MLP: Initialization \n",
      "\u001b[0m* Model 1: Sigmoid activation \n",
      "\u001b[0m* Model 2: ReLU activation. \n",
      "\u001b[0m* Model 3: Batch Normalization. \n",
      "\u001b[0m* Model 4 : Dropout. \n",
      "\u001b[0m* MNIST classification in Keras. \n",
      "\u001b[0m* Hyperparameter tuning in Keras. \n",
      "\u001b[0m* Exercise: Try different MLP architectures on MNIST dataset. \n",
      "**************************************************\n",
      "\u001b[1m Section 49 Deep Learning: Convolutional Neural Nets.  0/19\n",
      "\u001b[0m* Biological inspiration: Visual Cortex \n",
      "\u001b[0m* Convolution:Edge Detection on images. \n",
      "\u001b[0m* Convolution:Padding and strides \n",
      "\u001b[0m* Convolution over RGB images. \n",
      "\u001b[0m* Convolutional layer. \n",
      "\u001b[0m* Max-pooling. \n",
      "\u001b[0m* CNN Training: Optimization \n",
      "\u001b[0m* Example CNN: LeNet [1998] \n",
      "\u001b[0m* ImageNet dataset. \n",
      "\u001b[0m* Data Augmentation. \n",
      "\u001b[0m* Convolution Layers in Keras \n",
      "\u001b[0m* AlexNet \n",
      "\u001b[0m* VGGNet \n",
      "\u001b[0m* Residual Network. \n",
      "\u001b[0m* Inception Network. \n",
      "\u001b[0m* What is Transfer learning. \n",
      "\u001b[0m* Code example: Cats vs Dogs. \n",
      "\u001b[0m* Code Example: MNIST dataset. \n",
      "\u001b[0m* Assignment: Try various CNN networks on MNIST dataset. \n",
      "**************************************************\n",
      "\u001b[1m Section 50 Deep Learning: Long Short-term memory (LSTMs)  0/11\n",
      "\u001b[0m* Why RNNs? \n",
      "\u001b[0m* Recurrent Neural Network. \n",
      "\u001b[0m* Training RNNs: Backprop. \n",
      "\u001b[0m* Types of RNNs. \n",
      "\u001b[0m* Need for LSTM/GRU. \n",
      "\u001b[0m* LSTM. \n",
      "\u001b[0m* GRUs. \n",
      "\u001b[0m* Deep RNN. \n",
      "\u001b[0m* Bidirectional RNN. \n",
      "\u001b[0m* Code example : IMDB Sentiment classification \n",
      "\u001b[0m* Exercise: Amazon Fine Food reviews LSTM model. \n",
      "**************************************************\n",
      "\u001b[1m Section 51 Interview Questions on Deep Learning  0/1\n",
      "\u001b[0m* Questions and Answers \n",
      "**************************************************\n",
      "\u001b[1m Section 52 Case Study 9: Self Driving Car  0/13\n",
      "\u001b[0m* Self Driving Car :Problem definition. \n",
      "\u001b[0m* Datasets. \n",
      "\u001b[0m* Data understanding & Analysis :Files and folders. \n",
      "\u001b[0m* Dash-cam images and steering angles. \n",
      "\u001b[0m* Split the dataset: Train vs Test \n",
      "\u001b[0m* EDA: Steering angles \n",
      "\u001b[0m* Mean Baseline model: simple \n",
      "\u001b[0m* Deep-learning model:Deep Learning for regression: CNN, CNN+RNN \n",
      "\u001b[0m* Batch load the dataset. \n",
      "\u001b[0m* NVIDIA’s end to end  CNN model. \n",
      "\u001b[0m* Train the model. \n",
      "\u001b[0m* Test and visualize the output. \n",
      "\u001b[0m* Extensions. \n",
      "\u001b[0m* Assignment. \n",
      "**************************************************\n",
      "\u001b[1m Section 53 Case Study 10: Music Generation using Deep-Learning  0/11\n",
      "\u001b[0m* Real-world problem \n",
      "\u001b[0m* Music representation \n",
      "\u001b[0m* Char-RNN  with abc-notation :Char-RNN model \n",
      "\u001b[0m* Char-RNN  with abc-notation :Data preparation. \n",
      "\u001b[0m* Char-RNN  with abc-notation:Many to Many RNN ,TimeDistributed-Dense layer \n",
      "\u001b[0m* Char-RNN  with abc-notation : State full RNN \n",
      "\u001b[0m* Char-RNN  with abc-notation :Model architecture,Model training. \n",
      "\u001b[0m* Char-RNN  with abc-notation :Music generation. \n",
      "\u001b[0m* Char-RNN  with abc-notation :Generate tabla music \n",
      "\u001b[0m* MIDI music generation. \n",
      "\u001b[0m* Survey blog: \n",
      "**************************************************\n",
      "\u001b[1m Section 54 Case Study 11: Human Activity Recognition  0/8\n",
      "\u001b[0m* Human Activity Recognition Problem definition \n",
      "\u001b[0m* Dataset understanding \n",
      "\u001b[0m* Data cleaning & preprocessing \n",
      "\u001b[0m* EDA:Univariate analysis. \n",
      "\u001b[0m* EDA:Data visualization using t-SNE \n",
      "\u001b[0m* Classical ML models. \n",
      "\u001b[0m* Deep-learning Model. \n",
      "\u001b[0m* Exercise: Build deeper LSTM models and hyper-param tune them \n",
      "**************************************************\n",
      "\u001b[1m Section 55 Case Studies  0/4\n",
      "\u001b[0m* AD-Click Predicition \n",
      "\u001b[0m* Human Activity Recognition using smartphones \n",
      "\u001b[0m* Song similarity and genre classification \n",
      "\u001b[0m* Facebook Friend Recommendation using Graph Mining \n",
      "**************************************************\n",
      "\u001b[1m Section 56 Interview Questions  0/2\n",
      "\u001b[0m* Revision Questions \n",
      "\u001b[0m* Questions And Answers \n",
      "\u001b[0m* External resources for Interview Questions \n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(lst)):\n",
    "    print('*'*50)\n",
    "    header = lst[i].find('h4')\n",
    "    print(\"\\033[1m Section\",i,header.get_text().strip())\n",
    "    content = lst[i].findAll('a',{'class':'lesson-title course-item-title button-load-item'})\n",
    "    for i in content:\n",
    "        print('\\033[0m*'+i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = urllib.request.urlopen('http://www.deeplearningbook.org/').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(page,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_elements = soup.findAll('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_htmls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n",
      "none\n",
      "none\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "for i in list_elements:\n",
    "    try:\n",
    "        new_url = 'http://www.deeplearningbook.org/'+i.find('a')['href']\n",
    "    except:\n",
    "        print('none')\n",
    "    all_htmls.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = []\n",
    "for i in all_htmls:\n",
    "    chapter = urllib.request.urlopen(i).read()\n",
    "    chapters.append(chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
